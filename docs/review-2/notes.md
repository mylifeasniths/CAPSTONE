## Topics to learn

- Sequential
- conv2D
- MaxPooling2D
- activation function
  - Relu
  - softmax
- Dense Layer
- Flatten

<!--
## Activation Function
- Non Linear in nature
- Used in the hidden layers of a neural network. This allows the model to learn more complex functions than a network trained using a linear activation function.

## ReLU (Rectified Linear Unit)
Commonly used in Hidden Layer

## Softmax
- Commonly used in Output Layer
- Multiclass Classification -->
